# 通用类问题

1. 激活函数对于神经网络的意义，线性函数是否可以作为激活函数？sigmoid 的基本定义及导数
2. ReLU 的全程是什么？优缺点？写出几种常见 ReLU 的形式， PReLU 在反向传播中如何处理？
3. 简述 Softmax 的定义及意义。
4. 简述 L/A/Am-Softmax、GroupSoftMax 的定义及意义。
5. 解释 Focal Loss 与 Center Loss。
6. 模型过拟合问题该如何解决？
7. 梯度消失和梯度爆炸产生的原因是什么的？如何确定模型发生了梯度爆炸？有什么缓解办法？
8. 如何处理非平衡数据集？
9. 逻辑回归用于解决什么问题？以逻辑回归问题为例，训练时不进行输入特征归一化会导致无法收敛么？进行归一化的目的是什么？
10. 模型的 bias 与 variance 的区别？如何降低？
11. 梯度下降方法找到的一定是目标函数下降最快的方向么？如何理解 GD 与 SGD、minibatchSGD 算法的差异?
12. Batch size 的对于模型收敛的影响？Adam optimizer 与 SGD 的优缺点对比？
13. 学习率过大对训练过程有何影响？什么是学习率调度策略？如何选择？
14. batch normalization 的具体流程? 解决什么问题? 使用时注意事项?训练测试有和差别？
15. 解释 warmup
16. 解释 label smoothing
17. BN、LN、GN、IN 有何区别？AdaIN 的使用方法及作用
18. 池化层有何作用？
19. Dropout 的含义、目的，测试时和训练时有何区别？同一种模型结构训练时使用不同的 dropout 比率，测试时速度有何对应变化？
20. 梯度剪裁的含义及目的？ 
21. 模型参数初始化对训练结果是否有影响？常用的初始化策略是什么？
22. finetuning 的流程及注意事项（以分类任务为例）
23. 简述 AlexNet、VGGNet、GoogleNet 以及 ResNet、DenseNet 的结构和特点。
24. 简述 NiN 的核心思想，你知道的 1*1 卷积一般都有何作用？
25. 简述 FCN 的核心思想
26. 简述 Inception、Xception 及 MobileNet 的核心结构
27. 简述 EfficientNet 的核心思想
28. 简述 UNet 的基本结构
29. 简述 FPN 的基本结构
30. 简述 SENet 的核心结构
31. 什么是组卷积？简述 ResNext、ShuffleNet 基本结构
32. 什么是转置卷积？
33. 简述 Attention 及 Channel attention 的计算过程
34. 空洞卷积的结构以及存在的问题，什么是膨胀系数？
35. 什么是可变形卷积，如何实现，有何作用
36. 简述基于通道重要性评估的剪枝方法流程
37. 训练和测试原始 VGG19 网络时，为何要固定网络输入大小？如果使用了不同大小的输入图像，会出现什么问题？
38. ResNet18 有多少卷积层？
39. 卷积层参数跟什么有关？影响输出特征图大小的因素包括？
40. 对于 VGG16 来说，当图像尺寸变为 2 倍，卷积层的参数数量变为几倍？
41. 使用 1 次 7x7 卷积的模型和使用 3 次 5x5 卷积的模型在性能、感受野和参数量上是否一样？
42. cross-entropy loss, hinge loss, MSE loss 的公式？鲁棒性分别如何？
43. 分类任务为什么不使用 MSEloss？使用了会有什么问题？
44. Jaccob 矩阵，Hessian 矩阵和 Gram 矩阵的公式？
45. 怎么对模型效果进行评估的？当机器学习性能遭遇瓶颈时，你会如何优化？
46. 有什么解决模型过拟合的途径？
49. 图像相关问题训练时预处理(增强)手段有哪些? 应用场景各是怎么样
63. 什么是多示例学习
64. 什么是元学习
65. 什么是持续学习
66. 什么是半监督学习、什么是弱监督学习，典型应用
67. 什么是自监督学习
70. 什么是泛化、鲁棒
80. 什么是基于优化的方法，与 feedforward 方法有何区别，有什么典型应用
81. CNN 类结构与 Transformer 类结构有什么典型区别
82. 什么是 SOTA、vanilla、oracle、benchmark、head、neck、bottleneck、backbone、embedding、logits、pretext/downstream task
83. 什么是正确的炼丹流程！
