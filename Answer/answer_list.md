# Answers of [all topics](https://github.com/MsterDC/CVM-DL_Base/blob/main/topic.md) in deep learning

Please submit your answers here

## 🔖 Example

[5] 解释 Focal Loss 与 Center Loss
- [参考论文1](xxx.pdf) | [参考论文2](xxx.pdf) | [参考链接1](xxx.com) | [参考链接2](xxx.com)
```
1. Focal Loss
   one-stage目标检测尽管有着运算速度快的优点，但在性能方面往往低于two-stage的检测器。
   有人发现，导致这种情况的原因是单阶段目标检测器没有对proposal进行筛选，从而导致结果的前后景类别不平衡。
   大量的简单背景proposal尽管单个的loss较低，但累加起来却overwhelm高loss的难样本。
   Focal Loss在交叉熵loss前加上（1-pt)，pt为预测的logit。当样本越简单，pt越大，（1-pt）越小，loss越小，则简单样本的影响越低。
    
2. Center Loss
   以往使用CE进行训练的分类任务，往往只能得到一个分离的特征表示。
   center loss为每一个样本学习一个特征中心，使得每个类别的特征尽可能聚集，产生更加具有判别性的特征。
```

[11] 梯度下降方法找到的一定是目标函数下降最快的方向么？如何理解 GD 与 SGD、minibatchSGD 算法的差异?
```
1. 梯度下降方法找到的一定是目标函数下降最快的方向么？
   不是。
   它只是目标函数在当前的点的切平面上下降最快的方法，只有linear或subliner的速度。
   或者说，局部最优解的方向对于全局最优解来说不一定是最优的。
    
2. 如何理解 GD 与 SGD、minibatchSGD 算法的差异?
   GD是整个training set计算一次梯度，minibatchSGD是每个batch计算一次，SGD是每个样本计算一次。
   随着计算梯度的样本数增加，计算出来的梯度越接近于真实的梯度方向，但速度也越慢，同时没有了噪声的扰动，会难以走出局部最优解。
```


[14] batch normalization 的具体流程? 解决什么问题? 使用时注意事项?训练测试有和差别？
```
1. batch normalization 的具体流程?
   BN是一个先归一化，再逆归一化的过程。假设输入数据的维度是[B, C, W, H]。
   首先计算同一个batch内各个通道（b*w*h个数）的均值和方差，并对每个通道进行归一化。
   同时，每个通道有两个可训练的参数：再缩放参数α和再平移参数β，使用这个参数数据进行逆归一化。
    
2. 解决什么问题?
   从理论上来看，提出BN的文章指出，当模型每一层的数据分布不一致时，会导致训练的难度增加，BN就可以缓解这种分布不一致的现象。
   但后续的文章指出这种不一致是不存在的，真正导致BN生效的原因是L范数的平滑。
   不过BN的理论性还是一个广泛讨论的问题，没有真正的结论；
   从炼丹来看，BN加快网络的训练与收敛速度，控制梯度爆炸防止梯度消失。
   同时BN的计算是在batch层面，所以引入了噪声，缓解了过拟合现象。
   
3. 使用时注意事项?训练测试有和差别？
   需要注意batch的大小对于结果的影响。
   训练时，每个batch的均值和方差是根据输入的batch计算的，比较好获得。
   但是测试时只有一个输入，无法获得batch的数据。
   因此，训练时会每个通道维护一个全局的均值和方差，用于测试时使用。
```

[18] 池化层有何作用？
```
   特征不变性。由于池化层关注的是某一区域的特征而不是某一特征，所以存在容忍性，输入有微小偏差时不会导致特征的变化。
   特征降维。专人做专事，让卷积操作专心聚合特征，扩大感受野的任务由池化层来完成。
   在一定程度上防止过拟合。
```


[20] 梯度剪裁的含义及目的？
```
   当网络中的每一层的梯度累乘，随着层数的增加，梯度可能越来越到，最后导致梯度爆炸。
   梯度裁剪对于梯度设定一个范围，clip掉过大的梯度，从而防止梯度爆炸。
```

[70] 什么是泛化、鲁棒？
```
   鲁棒性：模型对于输入扰动或对抗样本的能力。
   泛化性：模型对于新数据做出准确预测的能力。
```


## 🎃 Answers

[1] xxx
- [xxx](yyy) | [xxx](yyy)
```
1. xxx
    - yyy
2. xxx
    - yyy
```

---

[2] xxx
- [xxx](yyy) | [xxx](yyy)

```
1. xxx
    - yyy
2. xxx
    - yyy
```

---


